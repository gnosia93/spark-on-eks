
### 1. api 서버 URL 확인 ###

```
$ kubectl cluster-info
Kubernetes control plane is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

### 2. 테스트 어플리케이션 실행 ###

쿠버네티스 클러스터에서 스파크 어플리케이션을 실행하기 위해 아래와 같이 spark-submit 명령어를 사용한다. 이때 쿠버네티스 api 주소 및 포트는 위에서 조회한 정보로 대체하고, 스파크 컨테이너 이미지의 경우 좀전에 생성한 AWS public ecr 레포지토리의 URL 을 입력한다. 



#### 로컬 PC 에서 실행하기 ####
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master local \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar

23/02/04 14:25:33 INFO SparkContext: Running Spark version 3.3.1
23/02/04 14:25:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 14:25:33 INFO ResourceUtils: ==============================================================
23/02/04 14:25:33 INFO ResourceUtils: No custom resources configured for spark.driver.
23/02/04 14:25:33 INFO ResourceUtils: ==============================================================
23/02/04 14:25:33 INFO SparkContext: Submitted application: Spark Pi
23/02/04 14:25:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)

...
...
23/02/04 14:25:36 INFO SparkUI: Stopped Spark web UI at http://192.168.29.29:4040
23/02/04 14:25:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/02/04 14:25:36 INFO MemoryStore: MemoryStore cleared
23/02/04 14:25:36 INFO BlockManager: BlockManager stopped
23/02/04 14:25:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/02/04 14:25:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/02/04 14:25:36 INFO SparkContext: Successfully stopped SparkContext
23/02/04 14:25:36 INFO ShutdownHookManager: Shutdown hook called
23/02/04 14:25:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-661afe02-687c-4710-96b0-b3113b5dbedf
23/02/04 14:25:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-5441fbe9-a6cc-45ed-9641-6557c54bdaa5
```

#### docker desktop 에서 실행하기 ####

docker-desktop 으로 k8s 컨텍스트를 변경 한 후, 클러스터 API 엔드포인트 정보를 받아온다. 
```
$ kubectl config use-context docker-desktop
Switched to context "docker-desktop".

$ kubectl config get-contexts
CURRENT   NAME                                             CLUSTER                                 AUTHINFO                                         NAMESPACE
*         docker-desktop                                   docker-desktop                          docker-desktop
          hopigaga@spark-on-eks.ap-northeast-2.eksctl.io   spark-on-eks.ap-northeast-2.eksctl.io   hopigaga@spark-on-eks.ap-northeast-2.eksctl.io

$ kubectl cluster-info
Kubernetes control plane is running at https://kubernetes.docker.internal:6443
CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

```
$ ./bin/spark-submit \
    --master k8s://https://kubernetes.docker.internal:6443 \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    local:///opt/spark/examples/jars/spark-examples_2.12-3.3.1.jar 
    
23/02/04 17:43:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 17:43:20 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/02/04 17:43:21 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
23/02/04 17:43:23 INFO LoggingPodStatusWatcherImpl: State changed, new state:
	 pod name: spark-pi-1763c6861b9822c2-driver
	 namespace: default
	 labels: spark-app-name -> spark-pi, spark-app-selector -> spark-4811dfc3d24f401181739dc41fcdc58a, spark-role -> driver, spark-version -> 3.3.1
	 pod uid: d1b643dc-3b65-46fa-85e7-7721f9f5ff3d
	 creation time: 2023-02-04T08:43:22Z
	 service account name: spark
	 volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dcq5x
	 node name: docker-desktop
	 start time: 2023-02-04T08:43:22Z
	 phase: Pending
	 container status:
		 container name: spark-kubernetes-driver
		 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
		 container state: waiting
		 pending reason: ContainerCreating
23/02/04 17:43:23 INFO LoggingPodStatusWatcherImpl: Waiting for application spark-pi with submission ID default:spark-pi-1763c6861b9822c2-driver to finish...
23/02/04 17:43:23 INFO LoggingPodStatusWatcherImpl: State changed, new state:
	 pod name: spark-pi-1763c6861b9822c2-driver
	 namespace: default
	 labels: spark-app-name -> spark-pi, spark-app-selector -> spark-4811dfc3d24f401181739dc41fcdc58a, spark-role -> driver, spark-version -> 3.3.1
	 pod uid: d1b643dc-3b65-46fa-85e7-7721f9f5ff3d
	 creation time: 2023-02-04T08:43:22Z
	 service account name: spark
	 volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dcq5x
	 node name: docker-desktop
	 start time: 2023-02-04T08:43:22Z
	 phase: Pending
	 container status:
		 container name: spark-kubernetes-driver
		 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
		 container state: waiting
		 pending reason: ContainerCreating
23/02/04 17:43:24 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Pending)
23/02/04 17:43:24 INFO LoggingPodStatusWatcherImpl: State changed, new state:
	 pod name: spark-pi-1763c6861b9822c2-driver
	 namespace: default
	 labels: spark-app-name -> spark-pi, spark-app-selector -> spark-4811dfc3d24f401181739dc41fcdc58a, spark-role -> driver, spark-version -> 3.3.1
	 pod uid: d1b643dc-3b65-46fa-85e7-7721f9f5ff3d
	 creation time: 2023-02-04T08:43:22Z
	 service account name: spark
	 volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dcq5x
	 node name: docker-desktop
	 start time: 2023-02-04T08:43:22Z
	 phase: Running
	 container status:
		 container name: spark-kubernetes-driver
		 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
		 container state: running
		 container started at: 2023-02-04T08:43:23Z
23/02/04 17:43:25 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:26 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:27 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:28 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:29 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:30 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:31 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:32 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:33 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:34 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:35 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:36 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:37 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:38 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:39 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:40 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:41 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:42 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:43 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:44 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:45 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:46 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:47 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:48 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:49 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:50 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:51 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:52 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:53 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:54 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:55 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:56 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:57 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:57 INFO LoggingPodStatusWatcherImpl: State changed, new state:
	 pod name: spark-pi-1763c6861b9822c2-driver
	 namespace: default
	 labels: spark-app-name -> spark-pi, spark-app-selector -> spark-4811dfc3d24f401181739dc41fcdc58a, spark-role -> driver, spark-version -> 3.3.1
	 pod uid: d1b643dc-3b65-46fa-85e7-7721f9f5ff3d
	 creation time: 2023-02-04T08:43:22Z
	 service account name: spark
	 volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dcq5x
	 node name: docker-desktop
	 start time: 2023-02-04T08:43:22Z
	 phase: Running
	 container status:
		 container name: spark-kubernetes-driver
		 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
		 container state: terminated
		 container started at: 2023-02-04T08:43:23Z
		 container finished at: 2023-02-04T08:43:56Z
		 exit code: 0
		 termination reason: Completed
23/02/04 17:43:58 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:59 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Running)
23/02/04 17:43:59 INFO LoggingPodStatusWatcherImpl: State changed, new state:
	 pod name: spark-pi-1763c6861b9822c2-driver
	 namespace: default
	 labels: spark-app-name -> spark-pi, spark-app-selector -> spark-4811dfc3d24f401181739dc41fcdc58a, spark-role -> driver, spark-version -> 3.3.1
	 pod uid: d1b643dc-3b65-46fa-85e7-7721f9f5ff3d
	 creation time: 2023-02-04T08:43:22Z
	 service account name: spark
	 volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dcq5x
	 node name: docker-desktop
	 start time: 2023-02-04T08:43:22Z
	 phase: Succeeded
	 container status:
		 container name: spark-kubernetes-driver
		 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
		 container state: terminated
		 container started at: 2023-02-04T08:43:23Z
		 container finished at: 2023-02-04T08:43:56Z
		 exit code: 0
		 termination reason: Completed
23/02/04 17:43:59 INFO LoggingPodStatusWatcherImpl: Application status for spark-4811dfc3d24f401181739dc41fcdc58a (phase: Succeeded)
23/02/04 17:43:59 INFO LoggingPodStatusWatcherImpl: Container final statuses:


	 container name: spark-kubernetes-driver
	 container image: public.ecr.aws/o5l1c9o9/spark-scala-container:latest
	 container state: terminated
	 container started at: 2023-02-04T08:43:23Z
	 container finished at: 2023-02-04T08:43:56Z
	 exit code: 0
	 termination reason: Completed
23/02/04 17:43:59 INFO LoggingPodStatusWatcherImpl: Application spark-pi with submission ID default:spark-pi-1763c6861b9822c2-driver finished
23/02/04 17:43:59 INFO ShutdownHookManager: Shutdown hook called
23/02/04 17:43:59 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-d71a0b06-55ea-4a67-9ccd-a1a7c33d32a6

$ kubectl get pods
NAME                               READY   STATUS      RESTARTS   AGE
spark-pi-1763c6861b9822c2-driver   0/1     Completed   0          49s
```

로컬에서 실행하는 경우, 스파크 잡에 제대로 실행되는 것을 확인할 수 있다.

#### EKS 클러스터에서 실행하기 ####
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master k8s://https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    local:///opt/spark/examples/jars/spark-examples_2.12-3.3.1.jar 

23/02/04 11:48:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 11:48:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/02/04 11:48:51 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
Exception in thread "main" org.apache.spark.SparkException: Please specify spark.kubernetes.file.upload.path property.
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:330)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:276)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
```
그러나, 클러스터 모드로 실행하는 경우 스파크 드라이버 컨테이너가 spark-examples_2.12-3.3.1.jar 를 찾지 못해서 오류가 발생한다. 


#### 해결방법 ####

아래와 같이 두가지 방식으로 해결 가능하다. 컨테이너 이미지에 jar 파일을 추가하는 경우 어플리케이션 코드(spark-examples_2.12-3.3.1.jar) 가 변경될떄 마다 이미지를 새롭게 빌드해야 한다.

#### 1. 컨테이너 이미지에 jar 파일 추가하기 ####
spark-examples_2.12-3.3.1.jar 파일을 컨테이너 이미지에 추가하고 실행한다. 

* https://stackoverflow.com/questions/50337296/how-to-deploy-spark-application-jar-file-to-kubernetes-cluster

```
$ cp ./kubernetes/dockerfiles/spark/Dockerfile .
$ vi Dockerfile 


$ docker buildx create --use

$ docker buildx build --push \
     --platform linux/amd64,linux/arm64 \
     -t public.ecr.aws/o5l1c9o9/spark-scala-container .

```

#### 2. S3에 있는 jar 파일로 실행하기 #### 

* https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716



#### [참고] 컨테이너 실행 오류 디버깅 ####

```
$ kubectl get pods
NAME                               READY   STATUS   RESTARTS   AGE
spark-pi-d2fa188616fa86ef-driver   0/1     Error    0          17m

$ kubectl describe pod spark-pi-d2fa188616fa86ef-driver
Name:             spark-pi-d2fa188616fa86ef-driver
Namespace:        default
Priority:         0
Service Account:  default
Node:             ip-192-168-21-146.ap-northeast-2.compute.internal/192.168.21.146
Start Time:       Fri, 03 Feb 2023 20:12:44 +0900
Labels:           spark-app-name=spark-pi
                  spark-app-selector=spark-707191ebc47449b8a43892eaa7b396c6
                  spark-role=driver
                  spark-version=3.3.1
Annotations:      kubernetes.io/psp: eks.privileged
Status:           Failed
IP:               192.168.27.143
IPs:
  IP:  192.168.27.143
Containers:
  spark-kubernetes-driver:
    Container ID:  containerd://e6d0c76ecbbfbf2ce65924b3f413eef40ab7a54da7dcf1cb29df00a9f67b830f
    Image:         public.ecr.aws/o5l1c9o9/spark-scala-container:latest
    Image ID:      public.ecr.aws/o5l1c9o9/spark-scala-container@sha256:6ca8222ff45d9fbc0e573b128e894531eee82e292f8aa4ca9b91d640851036da
    Ports:         7078/TCP, 7079/TCP, 4040/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      driver
      --properties-file
      /opt/spark/conf/spark.properties
      --class
      org.apache.spark.examples.SparkPi
      local://./examples/jars/spark-examples_2.12-3.3.1.jar
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 03 Feb 2023 20:13:06 +0900
      Finished:     Fri, 03 Feb 2023 20:13:06 +0900
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  1408Mi
    Requests:
      cpu:     1
      memory:  1408Mi
    Environment:
      SPARK_USER:                 soonbeom
      SPARK_APPLICATION_ID:       spark-707191ebc47449b8a43892eaa7b396c6
      SPARK_DRIVER_BIND_ADDRESS:   (v1:status.podIP)
      SPARK_LOCAL_DIRS:           /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea
      SPARK_CONF_DIR:             /opt/spark/conf
    Mounts:
      /opt/spark/conf from spark-conf-volume-driver (rw)
      /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea from spark-local-dir-1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px877 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  spark-local-dir-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  spark-conf-volume-driver:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-drv-b4d89c8616fa8c0f-conf-map
    Optional:  false
  kube-api-access-px877:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    17m   default-scheduler  Successfully assigned default/spark-pi-d2fa188616fa86ef-driver to ip-192-168-21-146.ap-northeast-2.compute.internal
  Warning  FailedMount  17m   kubelet            MountVolume.SetUp failed for volume "spark-conf-volume-driver" : configmap "spark-drv-b4d89c8616fa8c0f-conf-map" not found
  Normal   Pulling      17m   kubelet            Pulling image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest"
  Normal   Pulled       17m   kubelet            Successfully pulled image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest" in 20.071959467s
  Normal   Created      17m   kubelet            Created container spark-kubernetes-driver
  Normal   Started      17m   kubelet            Started container spark-kubernetes-driver

$ kubectl logs spark-pi-d2fa188616fa86ef-driver

```




## 참고자료 ##
* https://stackoverflow.com/questions/58222205/uber-jar-not-found-in-kubernetes-via-spark-submit
* https://spark.apache.org/docs/latest/running-on-kubernetes.html
* https://spark.apache.org/docs/latest/configuration.html
* https://sparkbyexamples.com/spark/spark-submit-command/
* [[k8s][Error] Arm-AMD CPU 사이 문제 해결 - 파드 생성 시 CrashLoopBackOff, exec user process caused: exec format error 문제
](https://kimjingo.tistory.com/221)
