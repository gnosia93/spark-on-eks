
### 1. api 서버 URL 확인 ###

```
$ kubectl cluster-info
Kubernetes control plane is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

### 2. 테스트 어플리케이션 실행 ###

쿠버네티스 클러스터에서 스파크 어플리케이션을 실행하기 위해 아래와 같이 spark-submit 명령어를 사용한다. 이때 쿠버네티스 api 주소 및 포트는 위에서 조회한 정보로 대체하고, 스파크 컨테이너 이미지의 경우 좀전에 생성한 AWS public ecr 레포지토리의 URL 을 입력한다. 



#### 로컬 PC 에서 실행하기 ####
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master local \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar

23/02/04 14:25:33 INFO SparkContext: Running Spark version 3.3.1
23/02/04 14:25:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 14:25:33 INFO ResourceUtils: ==============================================================
23/02/04 14:25:33 INFO ResourceUtils: No custom resources configured for spark.driver.
23/02/04 14:25:33 INFO ResourceUtils: ==============================================================
23/02/04 14:25:33 INFO SparkContext: Submitted application: Spark Pi
23/02/04 14:25:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/02/04 14:25:33 INFO ResourceProfile: Limiting resource is cpu
23/02/04 14:25:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/02/04 14:25:33 INFO SecurityManager: Changing view acls to: soonbeom
23/02/04 14:25:33 INFO SecurityManager: Changing modify acls to: soonbeom
23/02/04 14:25:33 INFO SecurityManager: Changing view acls groups to:
23/02/04 14:25:33 INFO SecurityManager: Changing modify acls groups to:
23/02/04 14:25:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(soonbeom); groups with view permissions: Set(); users  with modify permissions: Set(soonbeom); groups with modify permissions: Set()
23/02/04 14:25:33 INFO Utils: Successfully started service 'sparkDriver' on port 55653.
23/02/04 14:25:33 INFO SparkEnv: Registering MapOutputTracker
23/02/04 14:25:34 INFO SparkEnv: Registering BlockManagerMaster
23/02/04 14:25:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/02/04 14:25:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/02/04 14:25:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/02/04 14:25:34 INFO DiskBlockManager: Created local directory at /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/blockmgr-94c0c573-7520-4714-be2d-9ea31c9912c3
23/02/04 14:25:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/02/04 14:25:34 INFO SparkEnv: Registering OutputCommitCoordinator
23/02/04 14:25:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/02/04 14:25:34 INFO SparkContext: Added JAR file:/Users/soonbeom/spark-3.3.1-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.1.jar at spark://192.168.29.29:55653/jars/spark-examples_2.12-3.3.1.jar with timestamp 1675488333095
23/02/04 14:25:34 INFO Executor: Starting executor ID driver on host 192.168.29.29
23/02/04 14:25:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/02/04 14:25:34 INFO Executor: Fetching spark://192.168.29.29:55653/jars/spark-examples_2.12-3.3.1.jar with timestamp 1675488333095
23/02/04 14:25:34 INFO TransportClientFactory: Successfully created connection to /192.168.29.29:55653 after 38 ms (0 ms spent in bootstraps)
23/02/04 14:25:34 INFO Utils: Fetching spark://192.168.29.29:55653/jars/spark-examples_2.12-3.3.1.jar to /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-5441fbe9-a6cc-45ed-9641-6557c54bdaa5/userFiles-ad278492-b776-4593-8673-1732f4e0d37d/fetchFileTemp14640571965308470561.tmp
23/02/04 14:25:34 INFO Executor: Adding file:/private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-5441fbe9-a6cc-45ed-9641-6557c54bdaa5/userFiles-ad278492-b776-4593-8673-1732f4e0d37d/spark-examples_2.12-3.3.1.jar to class loader
23/02/04 14:25:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55655.
23/02/04 14:25:34 INFO NettyBlockTransferService: Server created on 192.168.29.29:55655
23/02/04 14:25:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/02/04 14:25:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.29.29, 55655, None)
23/02/04 14:25:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.29.29:55655 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.29.29, 55655, None)
23/02/04 14:25:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.29.29, 55655, None)
23/02/04 14:25:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.29.29, 55655, None)
23/02/04 14:25:35 INFO SparkContext: Starting job: reduce at SparkPi.scala:38
23/02/04 14:25:35 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 2 output partitions
23/02/04 14:25:35 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)
23/02/04 14:25:35 INFO DAGScheduler: Parents of final stage: List()
23/02/04 14:25:35 INFO DAGScheduler: Missing parents: List()
23/02/04 14:25:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents
23/02/04 14:25:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)
23/02/04 14:25:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)
23/02/04 14:25:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.29.29:55655 (size: 2.3 KiB, free: 434.4 MiB)
23/02/04 14:25:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
23/02/04 14:25:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1))
23/02/04 14:25:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
23/02/04 14:25:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.29.29, executor driver, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
23/02/04 14:25:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/02/04 14:25:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 965 bytes result sent to driver
23/02/04 14:25:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (192.168.29.29, executor driver, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
23/02/04 14:25:36 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
23/02/04 14:25:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 198 ms on 192.168.29.29 (executor driver) (1/2)
23/02/04 14:25:36 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 965 bytes result sent to driver
23/02/04 14:25:36 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 49 ms on 192.168.29.29 (executor driver) (2/2)
23/02/04 14:25:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
23/02/04 14:25:36 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.846 s
23/02/04 14:25:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/02/04 14:25:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/02/04 14:25:36 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.964176 s
Pi is roughly 3.1406957034785172
23/02/04 14:25:36 INFO SparkUI: Stopped Spark web UI at http://192.168.29.29:4040
23/02/04 14:25:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/02/04 14:25:36 INFO MemoryStore: MemoryStore cleared
23/02/04 14:25:36 INFO BlockManager: BlockManager stopped
23/02/04 14:25:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/02/04 14:25:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/02/04 14:25:36 INFO SparkContext: Successfully stopped SparkContext
23/02/04 14:25:36 INFO ShutdownHookManager: Shutdown hook called
23/02/04 14:25:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-661afe02-687c-4710-96b0-b3113b5dbedf
23/02/04 14:25:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/xj5kthtj7qxgwlnsxnt02bjc0000gr/T/spark-5441fbe9-a6cc-45ed-9641-6557c54bdaa5
```

EKS 클러스터에서 실행하기
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master k8s://https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar
```


**** 오류 ...

```
(base) soonbeom@bcd07468d10a spark-3.3.1-bin-hadoop3 % ./bin/spark-submit \
    --master k8s://https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar
23/02/04 11:48:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 11:48:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/02/04 11:48:51 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
Exception in thread "main" org.apache.spark.SparkException: Please specify spark.kubernetes.file.upload.path property.
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:330)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:276)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
```
* https://stackoverflow.com/questions/50337296/how-to-deploy-spark-application-jar-file-to-kubernetes-cluster
* https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716


#### [참고] 컨테이너 실행 오류 디버깅 ####

```
$ kubectl get pods
NAME                               READY   STATUS   RESTARTS   AGE
spark-pi-d2fa188616fa86ef-driver   0/1     Error    0          17m

$ kubectl describe pod spark-pi-d2fa188616fa86ef-driver
Name:             spark-pi-d2fa188616fa86ef-driver
Namespace:        default
Priority:         0
Service Account:  default
Node:             ip-192-168-21-146.ap-northeast-2.compute.internal/192.168.21.146
Start Time:       Fri, 03 Feb 2023 20:12:44 +0900
Labels:           spark-app-name=spark-pi
                  spark-app-selector=spark-707191ebc47449b8a43892eaa7b396c6
                  spark-role=driver
                  spark-version=3.3.1
Annotations:      kubernetes.io/psp: eks.privileged
Status:           Failed
IP:               192.168.27.143
IPs:
  IP:  192.168.27.143
Containers:
  spark-kubernetes-driver:
    Container ID:  containerd://e6d0c76ecbbfbf2ce65924b3f413eef40ab7a54da7dcf1cb29df00a9f67b830f
    Image:         public.ecr.aws/o5l1c9o9/spark-scala-container:latest
    Image ID:      public.ecr.aws/o5l1c9o9/spark-scala-container@sha256:6ca8222ff45d9fbc0e573b128e894531eee82e292f8aa4ca9b91d640851036da
    Ports:         7078/TCP, 7079/TCP, 4040/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      driver
      --properties-file
      /opt/spark/conf/spark.properties
      --class
      org.apache.spark.examples.SparkPi
      local://./examples/jars/spark-examples_2.12-3.3.1.jar
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 03 Feb 2023 20:13:06 +0900
      Finished:     Fri, 03 Feb 2023 20:13:06 +0900
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  1408Mi
    Requests:
      cpu:     1
      memory:  1408Mi
    Environment:
      SPARK_USER:                 soonbeom
      SPARK_APPLICATION_ID:       spark-707191ebc47449b8a43892eaa7b396c6
      SPARK_DRIVER_BIND_ADDRESS:   (v1:status.podIP)
      SPARK_LOCAL_DIRS:           /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea
      SPARK_CONF_DIR:             /opt/spark/conf
    Mounts:
      /opt/spark/conf from spark-conf-volume-driver (rw)
      /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea from spark-local-dir-1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px877 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  spark-local-dir-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  spark-conf-volume-driver:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-drv-b4d89c8616fa8c0f-conf-map
    Optional:  false
  kube-api-access-px877:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    17m   default-scheduler  Successfully assigned default/spark-pi-d2fa188616fa86ef-driver to ip-192-168-21-146.ap-northeast-2.compute.internal
  Warning  FailedMount  17m   kubelet            MountVolume.SetUp failed for volume "spark-conf-volume-driver" : configmap "spark-drv-b4d89c8616fa8c0f-conf-map" not found
  Normal   Pulling      17m   kubelet            Pulling image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest"
  Normal   Pulled       17m   kubelet            Successfully pulled image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest" in 20.071959467s
  Normal   Created      17m   kubelet            Created container spark-kubernetes-driver
  Normal   Started      17m   kubelet            Started container spark-kubernetes-driver

$ kubectl logs spark-pi-d2fa188616fa86ef-driver

```




## 참고자료 ##

* https://spark.apache.org/docs/latest/running-on-kubernetes.html
* https://spark.apache.org/docs/latest/configuration.html
* https://sparkbyexamples.com/spark/spark-submit-command/
* [[k8s][Error] Arm-AMD CPU 사이 문제 해결 - 파드 생성 시 CrashLoopBackOff, exec user process caused: exec format error 문제
](https://kimjingo.tistory.com/221)
