
### 1. api 서버 URL 확인 ###

```
$ kubectl cluster-info
Kubernetes control plane is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

### 2. 테스트 어플리케이션 실행 ###

쿠버네티스 클러스터에서 스파크 어플리케이션을 실행하기 위해 아래와 같이 spark-submit 명령어를 사용한다. 이때 쿠버네티스 api 주소 및 포트는 위에서 조회한 정보로 대체하고, 스파크 컨테이너 이미지의 경우 좀전에 생성한 AWS public ecr 레포지토리의 URL 을 입력한다. 



로컬에서 테스트하기
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master local
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar
```

EKS 클러스터에서 실행하기
```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit \
    --master k8s://https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar
```


**** 오류 ...

```
(base) soonbeom@bcd07468d10a spark-3.3.1-bin-hadoop3 % ./bin/spark-submit \
    --master k8s://https://D90FB9BFE96932CC78C3D6CE23033347.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/spark-scala-container:latest \
    ./examples/jars/spark-examples_2.12-3.3.1.jar
23/02/04 11:48:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/02/04 11:48:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/02/04 11:48:51 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
Exception in thread "main" org.apache.spark.SparkException: Please specify spark.kubernetes.file.upload.path property.
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:330)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:276)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
```
* https://stackoverflow.com/questions/50337296/how-to-deploy-spark-application-jar-file-to-kubernetes-cluster
* https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716


### 3. 디버깅 ###

```
$ kubectl get pods
NAME                               READY   STATUS   RESTARTS   AGE
spark-pi-d2fa188616fa86ef-driver   0/1     Error    0          17m

$ kubectl describe pod spark-pi-d2fa188616fa86ef-driver
Name:             spark-pi-d2fa188616fa86ef-driver
Namespace:        default
Priority:         0
Service Account:  default
Node:             ip-192-168-21-146.ap-northeast-2.compute.internal/192.168.21.146
Start Time:       Fri, 03 Feb 2023 20:12:44 +0900
Labels:           spark-app-name=spark-pi
                  spark-app-selector=spark-707191ebc47449b8a43892eaa7b396c6
                  spark-role=driver
                  spark-version=3.3.1
Annotations:      kubernetes.io/psp: eks.privileged
Status:           Failed
IP:               192.168.27.143
IPs:
  IP:  192.168.27.143
Containers:
  spark-kubernetes-driver:
    Container ID:  containerd://e6d0c76ecbbfbf2ce65924b3f413eef40ab7a54da7dcf1cb29df00a9f67b830f
    Image:         public.ecr.aws/o5l1c9o9/spark-scala-container:latest
    Image ID:      public.ecr.aws/o5l1c9o9/spark-scala-container@sha256:6ca8222ff45d9fbc0e573b128e894531eee82e292f8aa4ca9b91d640851036da
    Ports:         7078/TCP, 7079/TCP, 4040/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      driver
      --properties-file
      /opt/spark/conf/spark.properties
      --class
      org.apache.spark.examples.SparkPi
      local://./examples/jars/spark-examples_2.12-3.3.1.jar
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 03 Feb 2023 20:13:06 +0900
      Finished:     Fri, 03 Feb 2023 20:13:06 +0900
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  1408Mi
    Requests:
      cpu:     1
      memory:  1408Mi
    Environment:
      SPARK_USER:                 soonbeom
      SPARK_APPLICATION_ID:       spark-707191ebc47449b8a43892eaa7b396c6
      SPARK_DRIVER_BIND_ADDRESS:   (v1:status.podIP)
      SPARK_LOCAL_DIRS:           /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea
      SPARK_CONF_DIR:             /opt/spark/conf
    Mounts:
      /opt/spark/conf from spark-conf-volume-driver (rw)
      /var/data/spark-535c137c-2d59-4ada-a31c-95c5ac7e1cea from spark-local-dir-1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px877 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  spark-local-dir-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  spark-conf-volume-driver:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-drv-b4d89c8616fa8c0f-conf-map
    Optional:  false
  kube-api-access-px877:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    17m   default-scheduler  Successfully assigned default/spark-pi-d2fa188616fa86ef-driver to ip-192-168-21-146.ap-northeast-2.compute.internal
  Warning  FailedMount  17m   kubelet            MountVolume.SetUp failed for volume "spark-conf-volume-driver" : configmap "spark-drv-b4d89c8616fa8c0f-conf-map" not found
  Normal   Pulling      17m   kubelet            Pulling image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest"
  Normal   Pulled       17m   kubelet            Successfully pulled image "public.ecr.aws/o5l1c9o9/spark-scala-container:latest" in 20.071959467s
  Normal   Created      17m   kubelet            Created container spark-kubernetes-driver
  Normal   Started      17m   kubelet            Started container spark-kubernetes-driver

$ kubectl logs spark-pi-d2fa188616fa86ef-driver

```




## 참고자료 ##

* https://spark.apache.org/docs/latest/running-on-kubernetes.html
* https://spark.apache.org/docs/latest/configuration.html
* https://sparkbyexamples.com/spark/spark-submit-command/
* [[k8s][Error] Arm-AMD CPU 사이 문제 해결 - 파드 생성 시 CrashLoopBackOff, exec user process caused: exec format error 문제
](https://kimjingo.tistory.com/221)
