이 실습은 [An Example to Predict Customer Churn with Sparkify](https://towardsdatascience.com/a-tutorial-using-spark-for-big-data-an-example-to-predict-customer-churn-9078ac9a1e85) 를 기반으로 작성되었습니다.

  * https://medium.com/swlh/sparkify-churn-prediction-with-pyspark-on-big-data-c50157ee491c
  * https://github.com/angang-li/sparkify
  * https://www.kaggle.com/code/yukinagae/sparkify-project-churn-prediction/notebook
  * https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html
 
### 1. S3 에 데이터 파일 업로드 ###

```
$ cd; git clone https://github.com/gnosia93/spark-on-eks.git

$ cd spark-on-eks/dataset
```

s3 버킷을 만들고 분석 대상 데이터를 업로드 한다. (참고) [S3 명령어](https://darrengwon.tistory.com/438)
```
$ aws s3api create-bucket --bucket spark-on-eks-`whoami` \
  --region ap-northeast-2 \
  --create-bucket-configuration LocationConstraint=ap-northeast-2

$ aws s3api put-object --bucket spark-on-eks-`whoami` \
  --key raw/mini_sparkify_event_data.json.gz \
  --body mini_sparkify_event_data.json.gz

$ aws s3 cp sparkify_event_data.parquet \
  s3://spark-on-eks-`whoami`/raw/sparkify_event_data.parquet/ \
  --recursive

$ aws s3api list-objects-v2 --bucket spark-on-eks-`whoami`
{
    "Contents": [
        {
            "Key": "raw/mini_sparkify_event_data.json.gz",
            "LastModified": "2023-02-05T01:42:52+00:00",
            "ETag": "\"236aa3bb11ed39f38d7615b3ce709980\"",
            "Size": 10661110,
            "StorageClass": "STANDARD"
        },
        {
            "Key": "raw/sparkify_event_data.parquet/.part-00081-34e220a1-2bae-4ed5-95f3-f8723c9327e0-c000.snappy.parquet.crc",
            "LastModified": "2023-02-05T03:35:41+00:00",
            "ETag": "\"162485a7ab05a2e0d20a6b40954bc9b4\"",
            "Size": 60612,
            "StorageClass": "STANDARD"
        },
        ....
        
    ]
}
```

억세스 가능한지 확인한다. 해당 버킷은 public access 가 불가능 하고, 버킷의 owner 만 접근 가능한 상태이다. 
```
$ aws s3 cp s3://spark-on-eks-`whoami`/raw/mini_sparkify_event_data.json.gz ./s3_mini_sparkify_event_data.json.gz
```


### 2. 주피터 랩을 이용한 EDA 및 피처 엔지니어링 ##### 

로컬 PC 의 주피터 랩에서 스칼러로 구현된 노트북 어플리케이션을 테스트하고, 어떠한 전처리가 수행되는 지 확인한다.
노트북의 전처리 중 일부 코드를 InteliJ 를 이용하여 스파크 스칼라 어플리케이션로 개발하고, EKS 클러스터로 배포하여 동작여부를 확인 할 것이다.

https://github.com/gnosia93/spark-on-eks/blob/main/tutorial/notebook/sparky-analysis.ipynb




### 3. InteliJ로 스파크 어플리케이션 개발하기 ###

##### Read in full sparkify dataset #####
```
event_data = "s3n://udacity-dsnd/sparkify/sparkify_event_data.json" 
df = spark.read.json(event_data)
df.head()
```




##### [참고] S3에 있는 jar 파일로 실행하기 ##### 

* https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716



## 참고자료 ##
* https://stackoverflow.com/questions/55189673/how-to-add-customized-jar-in-jupyter-notebook-in-scala
* https://github.com/vericast/spylon-kernel/blob/master/examples/basic_example.ipynb    -- spylon_kernel 
* https://jhleeeme.github.io/read-aws-s3-data-on-spark/
* https://github.com/jadianes/spark-py-notebooks
* https://towardsdatascience.com/a-tutorial-using-spark-for-big-data-an-example-to-predict-customer-churn-9078ac9a1e85
* https://github.com/elifinspace/sparkify

