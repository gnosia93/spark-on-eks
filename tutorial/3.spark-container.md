### 1. 로컬 PC 에 스파크 설치 ###

https://spark.apache.org/downloads.html 으로 방문하여 로컬 PC 의 home 디렉토리에 스파크 바이너리를 설치한다. 

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/spark-download.png)
```
$ cd; wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz

$ tar xvfz spark-3.3.1-bin-hadoop3.tgz 

$ cd spark-3.3.1-bin-hadoop3
```

### 2. ecr 레포지토리 생성 ###

us-east-1 리전에 퍼블릭 레포지토리를 만든다. 프라이비트 레포지토리와 달리 퍼블릭의 경우 us-east-1 리전에만 생성이 가능하다.
```
$ aws ecr-public create-repository \
     --repository-name spark-scala-container \
     --region us-east-1     
{
    "repository": {
        "repositoryArn": "arn:aws:ecr-public::xxxxxxxxxxxx:repository/spark-scala-container",
        "registryId": "xxxxxxxxxxxx",
        "repositoryName": "spark-scala-container",
        "repositoryUri": "public.ecr.aws/o5l1c9o9/spark-scala-container",
        "createdAt": "2023-02-03T13:44:39.344000+09:00"
    },
    "catalogData": {}
}     
```


### 3. ecr 로그인 ###

docker login 명령어를 사용하여 AWS public erc 레포지토리에 로그인 한다. 패스워드는 아래와 같이 aws CLI 명령어로 가져온다.
```
$ aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws

Login Succeeded
```


### 4. 이미지 빌드 및 푸시 ###
```
$ cp ./kubernetes/dockerfiles/spark/Dockerfile .

$ docker buildx create --use

$ docker buildx build --load --push \
     --platform linux/amd64,linux/arm64 \
     -t public.ecr.aws/o5l1c9o9/spark-scala-container .

[+] Building 63.2s (33/33) FINISHED
 => [internal] load .dockerignore                                                                                     0.0s
 => => transferring context: 2B                                                                                       0.0s
 => [internal] load build definition from Dockerfile                                                                  0.0s
 => => transferring dockerfile: 2.45kB                                                                                0.0s
 => [linux/amd64 internal] load metadata for docker.io/library/openjdk:11-jre-slim                                    1.8s
 => [linux/arm64 internal] load metadata for docker.io/library/openjdk:11-jre-slim                                    1.8s
 => [internal] load build context                                                                                     0.0s
 => => transferring context: 69.31kB                                                                                  0.0s
 => [linux/amd64  1/13] FROM docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b  0.0s
 => => resolve docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b31f012ea29d842  0.0s
 => [linux/arm64  1/13] FROM docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b  0.0s
 => => resolve docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b31f012ea29d842  0.0s
 => CACHED [linux/arm64  2/13] RUN set -ex &&     sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources  0.0s
 => CACHED [linux/arm64  3/13] COPY jars /opt/spark/jars                                                              0.0s
 => CACHED [linux/arm64  4/13] COPY bin /opt/spark/bin                                                                0.0s
 => CACHED [linux/arm64  5/13] COPY sbin /opt/spark/sbin                                                              0.0s
 => CACHED [linux/arm64  6/13] COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/                                  0.0s
 => CACHED [linux/arm64  7/13] COPY kubernetes/dockerfiles/spark/decom.sh /opt/                                       0.0s
 => CACHED [linux/arm64  8/13] COPY examples /opt/spark/examples                                                      0.0s
 => CACHED [linux/arm64  9/13] COPY kubernetes/tests /opt/spark/tests                                                 0.0s
 => CACHED [linux/arm64 10/13] COPY data /opt/spark/data                                                              0.0s
 => CACHED [linux/arm64 11/13] WORKDIR /opt/spark/work-dir                                                            0.0s
 => CACHED [linux/arm64 12/13] RUN chmod g+w /opt/spark/work-dir                                                      0.0s
 => CACHED [linux/arm64 13/13] RUN chmod a+x /opt/decom.sh                                                            0.0s
 => CACHED [linux/amd64  2/13] RUN set -ex &&     sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources  0.0s
 => CACHED [linux/amd64  3/13] COPY jars /opt/spark/jars                                                              0.0s
 => CACHED [linux/amd64  4/13] COPY bin /opt/spark/bin                                                                0.0s
 => CACHED [linux/amd64  5/13] COPY sbin /opt/spark/sbin                                                              0.0s
 => CACHED [linux/amd64  6/13] COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/                                  0.0s
 => CACHED [linux/amd64  7/13] COPY kubernetes/dockerfiles/spark/decom.sh /opt/                                       0.0s
 => CACHED [linux/amd64  8/13] COPY examples /opt/spark/examples                                                      0.0s
 => CACHED [linux/amd64  9/13] COPY kubernetes/tests /opt/spark/tests                                                 0.0s
 => CACHED [linux/amd64 10/13] COPY data /opt/spark/data                                                              0.0s
 => CACHED [linux/amd64 11/13] WORKDIR /opt/spark/work-dir                                                            0.0s
 => CACHED [linux/amd64 12/13] RUN chmod g+w /opt/spark/work-dir                                                      0.0s
 => CACHED [linux/amd64 13/13] RUN chmod a+x /opt/decom.sh                                                            0.0s
 => exporting to image                                                                                               61.2s
 => => exporting layers                                                                                               5.9s
 => => exporting manifest sha256:ba66cd40390c5a9e326d5f52ac285e5900d19f4cad9ddaa1c8ca0b92a12e4196                     0.0s
 => => exporting config sha256:7daf78374d8d3f3c84878b09da35748c2cde1058af73a50de6341ac4bf6226a2                       0.0s
 => => exporting manifest sha256:e3bb883698181c232ba54c8638724e49a6ec8708bc3461f9eed022c4ee37fe5e                     0.0s
 => => exporting config sha256:2cd162294c59e0cf3470c0d1b528db22f818d687a7bcc5f5f2594ef74e2dd95c                       0.0s
 => => exporting manifest list sha256:4fca62825b83bde7c9aee4a495b7ac07c45edc60630aa5216c2550dca8ac9d3c                0.0s
 => => pushing layers                                                                                                52.3s
 => => pushing manifest for public.ecr.aws/o5l1c9o9/spark-scala-container:latest@sha256:4fca62825b83bde7c9aee4a495b7  2.9s
 => [auth] aws:: o5l1c9o9/spark-scala-container:pull,push token for public.ecr.aws                                    0.0s
```

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/ecr-docker-image.png)


### [참고] 이미지 및 레포지토리 삭제 ###

* ecr 이미지 삭제 
```
aws ecr-public batch-delete-image \
      --repository-name spark-scala-container \
      --image-ids imageTag=latest \
      --region us-east-1
```
* ecr 레포지토리 삭제 
```
aws ecr-public delete-repository \
      --repository-name spark-scala-container \
      --force \
      --region us-east-1
```


## 참고자료 ##

* https://velog.io/@baeyuna97/exec-user-process-caused-exec-format-error-%EC%97%90%EB%9F%AC%ED%95%B4%EA%B2%B0
* https://docs.docker.com/build/drivers/docker-container/
* https://docs.aws.amazon.com/AmazonECR/latest/public/getting-started-cli.html
* https://spark.apache.org/docs/latest/running-on-kubernetes.html
* https://gurumee92.tistory.com/311
* https://stackoverflow.com/questions/73933469/how-to-specify-where-to-push-using-docker-buildx-build-command
* https://stackoverflow.com/questions/72945407/how-do-i-import-and-run-a-multi-platform-oci-image-in-docker-for-macos
