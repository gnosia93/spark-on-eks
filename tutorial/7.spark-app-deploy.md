1. entry-point.sh 이상함 - 항상 클라이언트 모드 이고, spark-submit 의 파라미터가 제대로 전달되지 않음.
2. 컨테이너 노드에 캐싱되어 있는 스파크 컨테이너 이미지를 리프레쉬 할수 있는 방안이 필요 (ecr 은 업데이트 되었으나, refresh가 제대로 되지 않음)
3. spark submit 이 제대로 파리미터를 넘겨주는 지 모르겠음.
4. eks ec2 인스턴스 롤 --> s3 접근 (key 없이)

https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html


```
% jar tvf spark-kubernetes_2.12-3.3.1.jar | grep KubernetesClientApplication
  7484 Sat Oct 15 10:14:18 KST 2022 org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.class
(base) soonbeom@bcd07468d10a jars % pwd
/Users/soonbeom/spark-3.3.1-bin-hadoop3/jars
(base) soonbeom@bcd07468d10a jars %
```

--properties-file /opt/spark/conf/spark.properties --class SparkySpark local:////opt/spark/app/SparkySpark-assembly-0.1.0-SNAPSHOT.jar

### ecr 레포지토리 생성 ###

```
$ aws ecr-public create-repository \
     --repository-name sparky-spark-app \
     --region us-east-1   
{
    "repository": {
        "repositoryArn": "arn:aws:ecr-public::xxxxxxxxxxx:repository/sparky-spark-app",
        "registryId": "xxxxxxxxxxx",
        "repositoryName": "sparky-spark-app",
        "repositoryUri": "public.ecr.aws/o5l1c9o9/sparky-spark-app",
        "createdAt": "2023-02-06T11:07:55.946000+09:00"
    },
    "catalogData": {}
}     
```

### 도커 패키징 하기 ###

[Dockerfile]
```
FROM public.ecr.aws/o5l1c9o9/spark-scala-container:latest
ARG spark_uid=185

USER root 
RUN set -ex && mkdir -p /opt/spark/app

COPY SparkySpark-assembly-0.1.0-SNAPSHOT.jar /opt/spark/app
USER ${spark_uid}
```

spark-on-eks 디렉토리로 이동해서 위에 나와 있는 내용으로 Dockerfile 을 생성하고, AWS_ACCESS_KEY_ID 와 AWS_SECRET_ACCESS_KEY 설정한 후 
이미지를 빌드한다. (보안상 문제가 발생할 수 있다. --> 컨테이너의 환경 변수를 외부에서 조회할 수 가 있을듯 하다??)

```
$ cd; mkdir -p spark-on-eks/app; cd spark-on-eks/app

$ vi Dockerfile


$ aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws

$ cp /Users/soonbeom/IdeaProjects/SparkySpark/target/scala-2.13/SparkySpark-assembly-0.1.0-SNAPSHOT.jar \
  SparkySpark-assembly-0.1.0-SNAPSHOT.jar

$ docker buildx build --push \
     --platform linux/amd64,linux/arm64 \
     -t public.ecr.aws/o5l1c9o9/sparky-spark-app:latest .
```

### 도커 데스크탑 K8 에서 실행하기 ###

```
$ cd ; cd spark-3.3.1-bin-hadoop3

$ kubectl config use-context docker-desktop
Switched to context "docker-desktop".

$ export AWS_ACCESS_KEY_ID=`awk '{if($1 == "aws_access_key_id") print $3}' ~/.aws/credentials`
$ export AWS_SECRET_ACCESS_KEY=`awk '{if($1 == "aws_secret_access_key") print $3}' ~/.aws/credentials`
$ env | grep AWS
AWS_ACCESS_KEY_ID=xxxxx
AWS_SECRET_ACCESS_KEY=yyyyy

$ ./bin/spark-submit -v \
    --master k8s://https://kubernetes.docker.internal:6443 \
    --deploy-mode cluster \
    --name sparky-spark-app \
    --class SparkySpark \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/sparky-spark-app:latest \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.context=docker-desktop \
    --conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID \
    --conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY \
    --conf spark.kubernetes.container.image.pullPolicy=Always \
    local:////opt/spark/app/SparkySpark-assembly-0.1.0-SNAPSHOT.jar 
    
$ kubectl get pods | grep Error | awk '{ print $1 }' | xargs kubectl delete pod  
$ kubectl get pods | grep Completed | awk '{ print $1 }' | xargs kubectl delete pod 
$ kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c

$ cocker image prune -a
```




### eks 서비스 어카운트 설정 ###

파드의 스파크 어플리케이션이 s3 에 접근하기 위해서, 서비스 어카운트를 설정한다.





### eks 에서 실행하기 ###

```
$ kubectl config use-context name-of-account@spark-on-eks.ap-northeast-2.eksctl.io
Switched to context "name-of-account@spark-on-eks.ap-northeast-2.eksctl.io".

$ kubectl config get-contexts
CURRENT   NAME                                             CLUSTER                                 AUTHINFO                                         NAMESPACE
          docker-desktop                                   docker-desktop                          docker-desktop
*         name-of-account@spark-on-eks.ap-northeast-2.eksctl.io   spark-on-eks.ap-northeast-2.eksctl.io   name-of-account@spark-on-eks.ap-northeast-2.eksctl.io

$ kubectl cluster-info
Kubernetes control plane is running at https://FC91D79A06A89466662783481F8328C7.gr7.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://FC91D79A06A89466662783481F8328C7.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


$ cd ; cd spark-3.3.1-bin-hadoop3

$ ./bin/spark-submit -v \
    --master k8s://https://FC91D79A06A89466662783481F8328C7.gr7.ap-northeast-2.eks.amazonaws.com \
    --deploy-mode cluster \
    --name sparky-spark-app \
    --class SparkySpark \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=public.ecr.aws/o5l1c9o9/sparky-spark-app:latest \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.context=name-of-account@spark-on-eks.ap-northeast-2.eksctl.io  \
    local:////opt/spark/app/SparkySpark-assembly-0.1.0-SNAPSHOT.jar
    
$ kubectl get pods
NAME                               READY   STATUS      RESTARTS   AGE
spark-pi-b31214861bf0441e-driver   0/1     Completed   0          99s   
```

### spark/hadoop -> connect s3 ###
```
* Providing AWS_PROFILE when reading S3 files with Spark - https://stackoverflow.com/questions/67267448/providing-aws-profile-when-reading-s3-files-with-spark

* Get Spark to use your AWS credentials file for S3 - http://wrschneider.github.io/2019/02/02/spark-credentials-file.html

* https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authenticating_with_S3

s3a:// 의 경우 3가지 방식의 인증 체계를 지원한다.

1. acceess_key / screet key 를 파라미터로 넘겨주는 방법
2. fs.s3a.aws.credentials.provider 를 설정하는 방법  (환경 변수 또는 .aws/credential 에서 읽어온다.)
    - EC2 / EKS 등의 경우 서비스 롤을 부여
3. hadoop config 파일에 기록하는 방법.

??? 도커는 어떻게 설정하는 게 맞을까? ---> secret 에 등록 ??? 
```


## 레퍼런스 ##

* https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html

* [awk](https://reakwon.tistory.com/163)

* [S3 버킷 정책](https://inpa.tistory.com/entry/AWS-%F0%9F%93%9A-S3-%EB%B2%84%ED%82%B7-%EC%83%9D%EC%84%B1-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%8B%A4%EC%A0%84-%EA%B5%AC%EC%B6%95)
* https://stackoverflow.com/questions/69867171/dockerfile-mkdir-permission-denied
* https://tlsdmstn56.github.io/spark-on-kubernetes/

* [컨피그맵](https://arisu1000.tistory.com/27843)

* [시크릿](https://arisu1000.tistory.com/27844)
