이번 챕터에서는 K8S 클러스터에서 동작하게 될 스파크 컨테이너 이미지를 생성하고, ecr 에 푸시할 것이다.

### 1. 스파크 바이너리 다운로드 ###

https://spark.apache.org/downloads.html 으로 방문하여 로컬 PC 의 home 디렉토리에 스파크 바이너리를 설치한다. 

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/spark-download.png)
```
$ cd; wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz

$ tar xvfz spark-3.3.1-bin-hadoop3.tgz 

$ cd spark-3.3.1-bin-hadoop3
```

### 2. ecr 레포지토리 생성 ###

us-east-1 리전에 퍼블릭 레포지토리를 만든다. 프라이비트 레포지토리와 달리 퍼블릭의 경우 us-east-1 리전에만 생성이 가능하다.
```
$ aws ecr-public create-repository \
     --repository-name spark-scala-container \
     --region us-east-1     
{
    "repository": {
        "repositoryArn": "arn:aws:ecr-public::xxxxxxxxxxxx:repository/spark-scala-container",
        "registryId": "xxxxxxxxxxxx",
        "repositoryName": "spark-scala-container",
        "repositoryUri": "public.ecr.aws/o5l1c9o9/spark-scala-container",
        "createdAt": "2023-02-03T13:44:39.344000+09:00"
    },
    "catalogData": {}
}     
```


### 3. ecr 로그인 ###

docker login 명령어를 사용하여 AWS public erc 레포지토리에 로그인 한다. 패스워드는 아래와 같이 aws CLI 명령어로 가져온다.
```
$ aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws

Login Succeeded
```


### 4. 이미지 빌드 및 푸시 ###
```
$ cp ./kubernetes/dockerfiles/spark/Dockerfile .

$ docker buildx inspect
$ docker buildx create --use

$ docker buildx build --push \
     --platform linux/amd64,linux/arm64 \
     -t public.ecr.aws/o5l1c9o9/spark-scala-container:latest .

[+] Building 63.2s (33/33) FINISHED
 => [internal] load .dockerignore                                                                                     0.0s
 => => transferring context: 2B                                                                                       0.0s
 => [internal] load build definition from Dockerfile                                                                  0.0s
 => => transferring dockerfile: 2.45kB                                                                                0.0s
 => [linux/amd64 internal] load metadata for docker.io/library/openjdk:11-jre-slim                                    1.8s
 => [linux/arm64 internal] load metadata for docker.io/library/openjdk:11-jre-slim                                    1.8s
 => [internal] load build context                                                                                     0.0s
 => => transferring context: 69.31kB                                                                                  0.0s
 => [linux/amd64  1/13] FROM docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b  0.0s
 => => resolve docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b31f012ea29d842  0.0s
 => [linux/arm64  1/13] FROM docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b  0.0s
 => => resolve docker.io/library/openjdk:11-jre-slim@sha256:93af7df2308c5141a751c4830e6b6c5717db102b3b31f012ea29d842  0.0s
 => CACHED [linux/arm64  2/13] RUN set -ex &&     sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources  0.0s
 => CACHED [linux/arm64  3/13] COPY jars /opt/spark/jars                                                              0.0s
 => CACHED [linux/arm64  4/13] COPY bin /opt/spark/bin                                                                0.0s
 => CACHED [linux/arm64  5/13] COPY sbin /opt/spark/sbin                                                              0.0s
 => CACHED [linux/arm64  6/13] COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/                                  0.0s
 => CACHED [linux/arm64  7/13] COPY kubernetes/dockerfiles/spark/decom.sh /opt/                                       0.0s
 => CACHED [linux/arm64  8/13] COPY examples /opt/spark/examples                                                      0.0s
 => CACHED [linux/arm64  9/13] COPY kubernetes/tests /opt/spark/tests                                                 0.0s
 => CACHED [linux/arm64 10/13] COPY data /opt/spark/data                                                              0.0s
 => CACHED [linux/arm64 11/13] WORKDIR /opt/spark/work-dir                                                            0.0s
 => CACHED [linux/arm64 12/13] RUN chmod g+w /opt/spark/work-dir                                                      0.0s
 => CACHED [linux/arm64 13/13] RUN chmod a+x /opt/decom.sh                                                            0.0s
 => CACHED [linux/amd64  2/13] RUN set -ex &&     sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources  0.0s
 => CACHED [linux/amd64  3/13] COPY jars /opt/spark/jars                                                              0.0s
 => CACHED [linux/amd64  4/13] COPY bin /opt/spark/bin                                                                0.0s
 => CACHED [linux/amd64  5/13] COPY sbin /opt/spark/sbin                                                              0.0s
 => CACHED [linux/amd64  6/13] COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/                                  0.0s
 => CACHED [linux/amd64  7/13] COPY kubernetes/dockerfiles/spark/decom.sh /opt/                                       0.0s
 => CACHED [linux/amd64  8/13] COPY examples /opt/spark/examples                                                      0.0s
 => CACHED [linux/amd64  9/13] COPY kubernetes/tests /opt/spark/tests                                                 0.0s
 => CACHED [linux/amd64 10/13] COPY data /opt/spark/data                                                              0.0s
 => CACHED [linux/amd64 11/13] WORKDIR /opt/spark/work-dir                                                            0.0s
 => CACHED [linux/amd64 12/13] RUN chmod g+w /opt/spark/work-dir                                                      0.0s
 => CACHED [linux/amd64 13/13] RUN chmod a+x /opt/decom.sh                                                            0.0s
 => exporting to image                                                                                               61.2s
 => => exporting layers                                                                                               5.9s
 => => exporting manifest sha256:ba66cd40390c5a9e326d5f52ac285e5900d19f4cad9ddaa1c8ca0b92a12e4196                     0.0s
 => => exporting config sha256:7daf78374d8d3f3c84878b09da35748c2cde1058af73a50de6341ac4bf6226a2                       0.0s
 => => exporting manifest sha256:e3bb883698181c232ba54c8638724e49a6ec8708bc3461f9eed022c4ee37fe5e                     0.0s
 => => exporting config sha256:2cd162294c59e0cf3470c0d1b528db22f818d687a7bcc5f5f2594ef74e2dd95c                       0.0s
 => => exporting manifest list sha256:4fca62825b83bde7c9aee4a495b7ac07c45edc60630aa5216c2550dca8ac9d3c                0.0s
 => => pushing layers                                                                                                52.3s
 => => pushing manifest for public.ecr.aws/o5l1c9o9/spark-scala-container:latest@sha256:4fca62825b83bde7c9aee4a495b7  2.9s
 => [auth] aws:: o5l1c9o9/spark-scala-container:pull,push token for public.ecr.aws                                    0.0s
 
$ docker buildx imagetools inspect public.ecr.aws/o5l1c9o9/spark-scala-container:latest
Name:      public.ecr.aws/o5l1c9o9/spark-scala-container:latest
MediaType: application/vnd.docker.distribution.manifest.list.v2+json
Digest:    sha256:d8530f05fc2a8c5d0ef96535c82b51f2930f7c9e1a4fff4408172a7125ce134d

Manifests:
  Name:      public.ecr.aws/o5l1c9o9/spark-scala-container:latest@sha256:5892139f8323cf5b6f3b42e4cbf37225969b583a05604c9fa83acd362dd545dc
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/amd64

  Name:      public.ecr.aws/o5l1c9o9/spark-scala-container:latest@sha256:be83bf64af0ce20b6acd6337add5d044a921d5ac758d4f7ffec8b0b7c718b03c
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/arm64
```

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/ecr-docker-image.png)


#### [참고] 스파크 이미지용 Dockerfile ####

스파크 이미지 빌드에 사용된 Dockerfile 을 열어보면 spark-3.3.1-bin-hadoop3 디렉토리에 있는 파일들을 복사하고 있는 것을 볼 수 있다. (스파크 실행 엔진)
example 하위 디렉토리에는 spark-examples_2.12-3.3.1.jar 파일이 있는데, jar 파일안에 SparkPi class 가 패키징되어 있다 (챕터 5에서 동작 테스트 예정) 

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/spark-image-dockerfile.png)

```
$ ls -la examples/jars
drwxr-xr-x  4 soonbeom  staff      128  2  4 18:31 .
drwxr-xr-x  4 soonbeom  staff      128 10 15 19:32 ..
-rw-r--r--  1 soonbeom  staff    78803 10 15 19:32 scopt_2.12-3.7.1.jar
-rw-r--r--  1 soonbeom  staff  1567445 10 15 19:32 spark-examples_2.12-3.3.1.jar
```

#### [참고] 이미지 및 레포지토리 삭제 ####

* ecr 이미지 삭제 
```
aws ecr-public batch-delete-image \
      --repository-name spark-scala-container \
      --image-ids imageTag=latest \
      --region us-east-1
```
* ecr 레포지토리 삭제 
```
aws ecr-public delete-repository \
      --repository-name spark-scala-container \
      --force \
      --region us-east-1
```


## 참고자료 ##
* https://80000coding.oopy.io/54dc871d-30c9-46cb-b609-2e8831541b5e
* https://velog.io/@baeyuna97/exec-user-process-caused-exec-format-error-%EC%97%90%EB%9F%AC%ED%95%B4%EA%B2%B0
* https://docs.docker.com/build/drivers/docker-container/
* https://docs.aws.amazon.com/AmazonECR/latest/public/getting-started-cli.html
* https://spark.apache.org/docs/latest/running-on-kubernetes.html
* https://gurumee92.tistory.com/311
* https://stackoverflow.com/questions/73933469/how-to-specify-where-to-push-using-docker-buildx-build-command
* https://stackoverflow.com/questions/72945407/how-do-i-import-and-run-a-multi-platform-oci-image-in-docker-for-macos
