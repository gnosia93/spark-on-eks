### 0. ec2 키 생성 ###


### 1.kubectl 설치하기 ###

```
$ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.7/2022-10-31/bin/darwin/amd64/kubectl
$ chmod +x ./kubectl
$ mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
$ kubectl version --short --client
```

### 2. eksctl 설치하기 ###

```
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"
$ brew upgrade eksctl && { brew link --overwrite eksctl; } || { brew tap weaveworks/tap; brew install weaveworks/tap/eksctl; }
$ eksctl version
0.127.0
```


### 3. eks 클러스터 생성 ###

cluster.yaml 파일을 생성한다.
```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spark-on-eks
  region: ap-northeast-2

nodeGroups:
  - name: spark-ng-x86 
    instanceType: m6i.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
  - name: spark-ng-arm
    instanceType: m6g.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
```

아래 명령어를 이용하여 새로운 클러스터를 하나 생성한다.
```
$ eksctl utils update-coredns --cluster=spark-on-eks
$ eksctl utils update-kube-proxy --cluster=spark-on-eks --approve
$ eksctl utils update-aws-node --cluster=spark-on-eks --approve

$ eksctl create cluster -f cluster.yaml
```
VPC 를 비롯한 서브넷, 시큐리티 그룹 등과 같은 AWS 리소스가 자동으로 생성된다.

[참고] 클러스터 삭제 명령어
```
$ eksctl delete cluster -f cluster.yaml
```



### 4. 클러스터 생성 확인 ###
```
$ kubectl get nodes
NAME                                                STATUS   ROLES    AGE     VERSION
ip-192-168-31-233.ap-northeast-2.compute.internal   Ready    <none>   4m10s   v1.24.9-eks-49d8fe8
ip-192-168-60-56.ap-northeast-2.compute.internal    Ready    <none>   4m8s    v1.24.9-eks-49d8fe8
$
$ kubectl get pods -A
NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE
kube-system   aws-node-l7b4d            1/1     Running   0          4m28s
kube-system   aws-node-z8l8s            1/1     Running   0          4m26s
kube-system   coredns-dc4979556-mp8vm   1/1     Running   0          12m
kube-system   coredns-dc4979556-p547d   1/1     Running   0          12m
kube-system   kube-proxy-4lz65          1/1     Running   0          4m28s
kube-system   kube-proxy-j4fpj          1/1     Running   0          4m26s
```



### (Optional) 5. 노드 그룹 생성 ###

self-managed 노드그룹을 생성하고, 키페어(aws-kp)로 로그인이 가능이 가능하도록 한다.

https://aws.amazon.com/ec2/instance-types/t3/
```
$ eksctl create nodegroup \
  --cluster spark-on-eks \
  --name spark-ng-x86 \
  --node-type m6i.2xlarge	 \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 4 \
  --ssh-access \
  --managed=false \
  --ssh-public-key aws-kp

$ eksctl utils update-coredns --cluster=spark-on-eks
$ eksctl utils update-kube-proxy --cluster=spark-on-eks --approve
$ eksctl utils update-aws-node --cluster=spark-on-eks --approve
$ eksctl create nodegroup \
  --cluster spark-on-eks \
  --name spark-ng-arm \
  --node-type m6g.2xlarge 	 \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 4 \
  --ssh-access \
  --managed=false \
  --ssh-public-key aws-kp 
 
$ eksctl get nodegroup --cluster=spark-on-eks
CLUSTER		NODEGROUP	STATUS		CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID		ASG NAME							TYPE
spark-on-eks	ng-2560a3f5	ACTIVE		2023-02-02T07:16:10Z	2		2		2			m5.large	AL2_x86_64		eks-ng-2560a3f5-d4c3087d-c634-f390-4ab1-f51f5dbb8ada		managed
spark-on-eks	spark-ng	CREATE_COMPLETE	2023-02-02T10:32:08Z	1		4		3			t3.medium	ami-0387dcacac4a946f3	eksctl-spark-on-eks-nodegroup-spark-ng-NodeGroup-DSN8A8DG1ZR	unmanaged

$ kubectl get nodes
NAME                                                STATUS   ROLES    AGE     VERSION
ip-192-168-21-146.ap-northeast-2.compute.internal   Ready    <none>   15m     v1.24.9-eks-49d8fe8
ip-192-168-31-233.ap-northeast-2.compute.internal   Ready    <none>   3h36m   v1.24.9-eks-49d8fe8
ip-192-168-56-158.ap-northeast-2.compute.internal   Ready    <none>   15m     v1.24.9-eks-49d8fe8
ip-192-168-60-56.ap-northeast-2.compute.internal    Ready    <none>   3h36m   v1.24.9-eks-49d8fe8
ip-192-168-94-56.ap-northeast-2.compute.internal    Ready    <none>   15m     v1.24.9-eks-49d8fe8
```

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-nodegroup-1.png)

### (Optional) 6. 노드그룹 교체 ###
```
$ eksctl delete nodegroup --cluster spark-on-eks --name ng-2560a3f5
2023-02-02 19:55:50 [ℹ]  1 nodegroup (ng-2560a3f5) was included (based on the include/exclude rules)
2023-02-02 19:55:50 [ℹ]  will drain 1 nodegroup(s) in cluster "spark-on-eks"
2023-02-02 19:55:50 [ℹ]  starting parallel draining, max in-flight of 1
2023-02-02 19:55:50 [ℹ]  cordon node "ip-192-168-31-233.ap-northeast-2.compute.internal"
2023-02-02 19:55:50 [ℹ]  cordon node "ip-192-168-60-56.ap-northeast-2.compute.internal"
2023-02-02 19:56:01 [✔]  drained all nodes: [ip-192-168-31-233.ap-northeast-2.compute.internal ip-192-168-60-56.ap-northeast-2.compute.internal]
2023-02-02 19:56:01 [ℹ]  will delete 1 nodegroups from cluster "spark-on-eks"
2023-02-02 19:56:02 [ℹ]  1 task: { 1 task: { delete nodegroup "ng-2560a3f5" [async] } }
2023-02-02 19:56:03 [ℹ]  will delete stack "eksctl-spark-on-eks-nodegroup-ng-2560a3f5"
2023-02-02 19:56:03 [ℹ]  will delete 0 nodegroups from auth ConfigMap in cluster "spark-on-eks"
2023-02-02 19:56:03 [✔]  deleted 1 nodegroup(s) from cluster "spark-on-eks"

$ kubectl get nodes -o wide
NAME                                                STATUS   ROLES    AGE   VERSION               INTERNAL-IP      EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                 CONTAINER-RUNTIME
ip-192-168-21-146.ap-northeast-2.compute.internal   Ready    <none>   22m   v1.24.9-eks-49d8fe8   192.168.21.146   54.180.99.166   Amazon Linux 2   5.4.228-131.415.amzn2.x86_64   containerd://1.6.6
ip-192-168-56-158.ap-northeast-2.compute.internal   Ready    <none>   22m   v1.24.9-eks-49d8fe8   192.168.56.158   52.78.166.5     Amazon Linux 2   5.4.228-131.415.amzn2.x86_64   containerd://1.6.6
ip-192-168-94-56.ap-northeast-2.compute.internal    Ready    <none>   22m   v1.24.9-eks-49d8fe8   192.168.94.56    3.38.147.111    Amazon Linux 2   5.4.228-131.415.amzn2.x86_64   containerd://1.6.6

$ kubectl get pods -A -o wide
NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE     IP               NODE                                                NOMINATED NODE   READINESS GATES
kube-system   aws-node-8gv88            1/1     Running   0          23m     192.168.21.146   ip-192-168-21-146.ap-northeast-2.compute.internal   <none>           <none>
kube-system   aws-node-pbpwm            1/1     Running   0          23m     192.168.94.56    ip-192-168-94-56.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-qvrxd            1/1     Running   0          23m     192.168.56.158   ip-192-168-56-158.ap-northeast-2.compute.internal   <none>           <none>
kube-system   coredns-dc4979556-6zllg   1/1     Running   0          4m57s   192.168.74.110   ip-192-168-94-56.ap-northeast-2.compute.internal    <none>           <none>
kube-system   coredns-dc4979556-nv7rq   1/1     Running   0          4m57s   192.168.52.85    ip-192-168-56-158.ap-northeast-2.compute.internal   <none>           <none>
kube-system   kube-proxy-48gdt          1/1     Running   0          23m     192.168.56.158   ip-192-168-56-158.ap-northeast-2.compute.internal   <none>           <none>
kube-system   kube-proxy-d7r62          1/1     Running   0          23m     192.168.94.56    ip-192-168-94-56.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-s4zpz          1/1     Running   0          23m     192.168.21.146   ip-192-168-21-146.ap-northeast-2.compute.internal   <none>           <none>
```

self-managed 의 경우, 노드 그룹 리스트에 정보가 나오지 않는다.

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-nodegroup-2.png)

## 참고자료 ##
* https://eksctl.io/usage/creating-and-managing-clusters/
* https://stackoverflow.com/questions/64059038/the-results-of-aws-eks-list-nodegroups-and-eksctl-get-nodegroups-are-inconsi
* https://findstar.pe.kr/2022/08/21/which-instance-type-is-right-for-EKS/
