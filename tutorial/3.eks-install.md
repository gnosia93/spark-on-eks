### 1. ec2 키페어 생성 ###

https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/create-key-pairs.html 를 참고하여 aws-kp 라는 이름의 키페이를 생성한다.

### 2.kubectl 설치하기 ###

```
$ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.7/2022-10-31/bin/darwin/amd64/kubectl
$ chmod +x ./kubectl
$ mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
$ kubectl version --short --client
```

### 3. eksctl 설치하기 ###

```
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"
$ brew upgrade eksctl && { brew link --overwrite eksctl; } || { brew tap weaveworks/tap; brew install weaveworks/tap/eksctl; }
$ eksctl version
0.127.0
```


### 4. eks 클러스터 생성 ###

cluster.yaml 파일을 생성한다.
```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spark-on-eks
  region: ap-northeast-2

nodeGroups:
  - name: spark-ng-x86 
    instanceType: m6i.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
  - name: spark-ng-arm
    instanceType: m6g.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
```

아래 명령어를 이용하여 새로운 클러스터를 하나 생성한다.
```
$ eksctl utils update-coredns --cluster=spark-on-eks
$ eksctl utils update-kube-proxy --cluster=spark-on-eks --approve
$ eksctl utils update-aws-node --cluster=spark-on-eks --approve

$ eksctl create cluster -f cluster.yaml

2023-02-04 20:16:15 [ℹ]  eksctl version 0.127.0
2023-02-04 20:16:15 [ℹ]  using region ap-northeast-2
2023-02-04 20:16:16 [ℹ]  setting availability zones to [ap-northeast-2c ap-northeast-2a ap-northeast-2b]
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2c - public:192.168.0.0/19 private:192.168.96.0/19
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2a - public:192.168.32.0/19 private:192.168.128.0/19
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2b - public:192.168.64.0/19 private:192.168.160.0/19
2023-02-04 20:16:16 [ℹ]  nodegroup "spark-ng-x86" will use "ami-0387dcacac4a946f3" [AmazonLinux2/1.24]
2023-02-04 20:16:16 [ℹ]  using EC2 key pair "aws-kp"
2023-02-04 20:16:16 [ℹ]  nodegroup "spark-ng-arm" will use "ami-01ff81a759a85a0f9" [AmazonLinux2/1.24]
2023-02-04 20:16:16 [ℹ]  using EC2 key pair "aws-kp"
2023-02-04 20:16:16 [ℹ]  using Kubernetes version 1.24
2023-02-04 20:16:16 [ℹ]  creating EKS cluster "spark-on-eks" in "ap-northeast-2" region with un-managed nodes
2023-02-04 20:16:16 [ℹ]  2 nodegroups (spark-ng-arm, spark-ng-x86) were included (based on the include/exclude rules)
2023-02-04 20:16:16 [ℹ]  will create a CloudFormation stack for cluster itself and 2 nodegroup stack(s)
2023-02-04 20:16:16 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2023-02-04 20:16:16 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --cluster=spark-on-eks'
2023-02-04 20:16:16 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "spark-on-eks" in "ap-northeast-2"
2023-02-04 20:16:16 [ℹ]  CloudWatch logging will not be enabled for cluster "spark-on-eks" in "ap-northeast-2"
2023-02-04 20:16:16 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=spark-on-eks'
2023-02-04 20:16:16 [ℹ]
2 sequential tasks: { create cluster control plane "spark-on-eks",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        2 parallel sub-tasks: {
            create nodegroup "spark-ng-x86",
            create nodegroup "spark-ng-arm",
        },
    }
}
2023-02-04 20:16:16 [ℹ]  building cluster stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:16:17 [ℹ]  deploying stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:16:47 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:17:18 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:18:19 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"

2023-02-04 20:29:25 [ℹ]  building nodegroup stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:25 [ℹ]  building nodegroup stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  deploying stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  deploying stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:25 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:56 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"

2023-02-04 20:34:19 [ℹ]  waiting for the control plane to become ready
2023-02-04 20:34:19 [✔]  saved kubeconfig as "/Users/soonbeom/.kube/config"
2023-02-04 20:34:19 [ℹ]  no tasks
2023-02-04 20:34:19 [✔]  all EKS cluster resources for "spark-on-eks" have been created
2023-02-04 20:34:19 [ℹ]  adding identity "arn:aws:iam::509076023497:role/eksctl-spark-on-eks-nodegroup-spa-NodeInstanceRole-1H8KFQM987K7X" to auth ConfigMap
2023-02-04 20:34:19 [ℹ]  nodegroup "spark-ng-x86" has 0 node(s)
2023-02-04 20:34:19 [ℹ]  waiting for at least 2 node(s) to become ready in "spark-ng-x86"
2023-02-04 20:35:10 [ℹ]  nodegroup "spark-ng-x86" has 3 node(s)
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-33-80.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-8-112.ap-northeast-2.compute.internal" is not ready
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-80-19.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:10 [ℹ]  adding identity "arn:aws:iam::509076023497:role/eksctl-spark-on-eks-nodegroup-spa-NodeInstanceRole-FIYEKMFETV5I" to auth ConfigMap
2023-02-04 20:35:10 [ℹ]  nodegroup "spark-ng-arm" has 0 node(s)
2023-02-04 20:35:10 [ℹ]  waiting for at least 2 node(s) to become ready in "spark-ng-arm"
2023-02-04 20:35:42 [ℹ]  nodegroup "spark-ng-arm" has 3 node(s)
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-22-60.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-35-245.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-78-81.ap-northeast-2.compute.internal" is not ready
2023-02-04 20:35:43 [ℹ]  kubectl command should work with "/Users/soonbeom/.kube/config", try 'kubectl get nodes'
2023-02-04 20:35:44 [✔]  EKS cluster "spark-on-eks" in "ap-northeast-2" region is ready
```
VPC 를 비롯한 서브넷, 시큐리티 그룹 등과 같은 AWS 리소스가 자동으로 생성된다.

[참고] 클러스터 삭제 명령어
```
$ eksctl delete cluster -f cluster.yaml
```





### 5. 클러스터 생성 확인 ###
```
$ kubectl config get-contexts
CURRENT   NAME                                             CLUSTER                                 AUTHINFO                                         NAMESPACE
          docker-desktop                                   docker-desktop                          docker-desktop
*         name-of-account@spark-on-eks.ap-northeast-2.eksctl.io   spark-on-eks.ap-northeast-2.eksctl.io   

$ kubectl get nodes
NAME                                                STATUS   ROLES    AGE     VERSION
ip-192-168-22-60.ap-northeast-2.compute.internal    Ready    <none>   3m15s   v1.24.9-eks-49d8fe8
ip-192-168-33-80.ap-northeast-2.compute.internal    Ready    <none>   4m9s    v1.24.9-eks-49d8fe8
ip-192-168-35-245.ap-northeast-2.compute.internal   Ready    <none>   3m17s   v1.24.9-eks-49d8fe8
ip-192-168-78-81.ap-northeast-2.compute.internal    Ready    <none>   3m19s   v1.24.9-eks-49d8fe8
ip-192-168-8-112.ap-northeast-2.compute.internal    Ready    <none>   4m8s    v1.24.9-eks-49d8fe8
ip-192-168-80-19.ap-northeast-2.compute.internal    Ready    <none>   4m8s    v1.24.9-eks-49d8fe8

$ kubectl get pods -o wide -A
NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE     IP               NODE                                                NOMINATED NODE   READINESS GATES
kube-system   aws-node-2ctq9            1/1     Running   0          4m26s   192.168.80.19    ip-192-168-80-19.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-5g79d            1/1     Running   0          3m33s   192.168.22.60    ip-192-168-22-60.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-5mmgm            1/1     Running   0          4m26s   192.168.8.112    ip-192-168-8-112.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-9xlzx            1/1     Running   0          3m37s   192.168.78.81    ip-192-168-78-81.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-krqth            1/1     Running   0          4m27s   192.168.33.80    ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-xxw47            1/1     Running   0          3m35s   192.168.35.245   ip-192-168-35-245.ap-northeast-2.compute.internal   <none>           <none>
kube-system   coredns-dc4979556-7zhhh   1/1     Running   0          15m     192.168.62.173   ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   coredns-dc4979556-hgkv9   1/1     Running   0          15m     192.168.50.214   ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-5jlgq          1/1     Running   0          4m26s   192.168.8.112    ip-192-168-8-112.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-8q7rx          1/1     Running   0          4m27s   192.168.33.80    ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-mdn6w          1/1     Running   0          3m33s   192.168.22.60    ip-192-168-22-60.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-mtq7n          1/1     Running   0          3m35s   192.168.35.245   ip-192-168-35-245.ap-northeast-2.compute.internal   <none>           <none>
kube-system   kube-proxy-qzvrh          1/1     Running   0          3m37s   192.168.78.81    ip-192-168-78-81.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-wz7jd          1/1     Running   0          4m26s   192.168.80.19    ip-192-168-80-19.ap-northeast-2.compute.internal    <none>           <none>
```

self-managed 노드 그룹의 경우, AWS 콘솔의 노드 그룹 리스트에 정보가 나오지 않는다.

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-cluster.png)





### 6. 성능 메트릭 및 로그 수집 ###
* https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html

성능 메트릭 및 켄테이너 로그를 수집하기 위해서는 아래와 두 단계에 걸쳐 설정 작업이 필요하다 

- EC2 서비스 메뉴에서 EKS 워커 노드 하나를 선택한 다음, attach 된 IAM Role 을 찾아 CloudWatchAgentServerPolicy 정책을 추가한다.  
- cloudwatch 및 flient bit 에이전트를 워커로드에 데몬셋으로 설치한다.  

클러스터, 노드 및 파드 별로 메트릭 정보와 로그 정보를 수지하기 위해서 cloudwatch 에이전트와 C 언어로 강량화 구현된 fluent bit 에이전트를 파드 형태로 설치할 것이다.  
```
$ ClusterName=spark-on-eks
RegionName=ap-northeast-2
FluentBitHttpPort='2020'
FluentBitReadFromHead='Off'
[[ ${FluentBitReadFromHead} = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'
[[ -z ${FluentBitHttpPort} ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On'
curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights\
/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring\
/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/'${ClusterName}'/;s/{{region_name}}/'${RegionName}'/;s/{{http_server_toggle}}/"'${FluentBitHttpServer}'"/;s/{{http_server_port}}/"'${FluentBitHttpPort}'"/;s/{{read_from_head}}/"'${FluentBitReadFromHead}'"/;s/{{read_from_tail}}/"'${FluentBitReadFromTail}'"/' | kubectl apply -f - 

$ % kubectl get namespace amazon-cloudwatch
NAME                STATUS   AGE
amazon-cloudwatch   Active   6h52m

$ kubectl get pods -n amazon-cloudwatch

NAME                     READY   STATUS    RESTARTS   AGE
cloudwatch-agent-85fcl   1/1     Running   0          6h46m
cloudwatch-agent-dsqn6   1/1     Running   0          6h46m
cloudwatch-agent-gdsgc   1/1     Running   0          6h46m
cloudwatch-agent-kbnl8   1/1     Running   0          6h46m
cloudwatch-agent-mmbxd   1/1     Running   0          6h46m
cloudwatch-agent-z4tnc   1/1     Running   0          6h46m
fluent-bit-46pfl         1/1     Running   0          6h46m
fluent-bit-6tcnp         1/1     Running   0          6h45m
fluent-bit-hfbfs         1/1     Running   0          6h46m
fluent-bit-qvshq         1/1     Running   0          6h45m
fluent-bit-v67m2         1/1     Running   0          6h45m
fluent-bit-vg2x5         1/1     Running   0          6h46m

$ kubectl describe pod fluent-bit-46pfl -n amazon-cloudwatch

$ kubectl describe configmaps fluent-bit-cluster-info -n amazon-cloudwatch
```

cloudwatch 의 container insight 메뉴에서 메트릭을 확인한다. 
![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-container-insight.png)

cloudwatch 의 logs 하단의 log groups 메뉴에서 어플리케이션 로그를 확인한다. 
![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-log-groups.png)



## 참고자료 ##
* https://eksctl.io/usage/creating-and-managing-clusters/
* https://stackoverflow.com/questions/64059038/the-results-of-aws-eks-list-nodegroups-and-eksctl-get-nodegroups-are-inconsi
* https://findstar.pe.kr/2022/08/21/which-instance-type-is-right-for-EKS/
* https://lannex.github.io/blog/2020/EFK-Fluentd-Fluentbit/
