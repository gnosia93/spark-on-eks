### 1. ec2 키페어 생성 ###

https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/create-key-pairs.html 를 참고하여 aws-kp 라는 이름의 키페이를 생성한다.

### 2.kubectl 설치하기 ###

```
$ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.7/2022-10-31/bin/darwin/amd64/kubectl
$ chmod +x ./kubectl
$ mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
$ kubectl version --short --client
```

### 3. eksctl 설치하기 ###

```
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"
$ brew upgrade eksctl && { brew link --overwrite eksctl; } || { brew tap weaveworks/tap; brew install weaveworks/tap/eksctl; }
$ eksctl version
0.127.0
```


### 4. eks 클러스터 생성 ###

cluster.yaml 파일을 생성한다.
```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spark-on-eks
  region: ap-northeast-2

nodeGroups:
  - name: spark-ng-x86 
    instanceType: m6i.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
  - name: spark-ng-arm
    instanceType: m6g.2xlarge
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    volumeSize: 80
    ssh: # use existing EC2 key
      publicKeyName: aws-kp
```

아래 명령어를 이용하여 새로운 클러스터를 하나 생성한다.
```
$ eksctl utils update-coredns --cluster=spark-on-eks
$ eksctl utils update-kube-proxy --cluster=spark-on-eks --approve
$ eksctl utils update-aws-node --cluster=spark-on-eks --approve

$ eksctl create cluster -f cluster.yaml

2023-02-04 20:16:15 [ℹ]  eksctl version 0.127.0
2023-02-04 20:16:15 [ℹ]  using region ap-northeast-2
2023-02-04 20:16:16 [ℹ]  setting availability zones to [ap-northeast-2c ap-northeast-2a ap-northeast-2b]
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2c - public:192.168.0.0/19 private:192.168.96.0/19
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2a - public:192.168.32.0/19 private:192.168.128.0/19
2023-02-04 20:16:16 [ℹ]  subnets for ap-northeast-2b - public:192.168.64.0/19 private:192.168.160.0/19
2023-02-04 20:16:16 [ℹ]  nodegroup "spark-ng-x86" will use "ami-0387dcacac4a946f3" [AmazonLinux2/1.24]
2023-02-04 20:16:16 [ℹ]  using EC2 key pair "aws-kp"
2023-02-04 20:16:16 [ℹ]  nodegroup "spark-ng-arm" will use "ami-01ff81a759a85a0f9" [AmazonLinux2/1.24]
2023-02-04 20:16:16 [ℹ]  using EC2 key pair "aws-kp"
2023-02-04 20:16:16 [ℹ]  using Kubernetes version 1.24
2023-02-04 20:16:16 [ℹ]  creating EKS cluster "spark-on-eks" in "ap-northeast-2" region with un-managed nodes
2023-02-04 20:16:16 [ℹ]  2 nodegroups (spark-ng-arm, spark-ng-x86) were included (based on the include/exclude rules)
2023-02-04 20:16:16 [ℹ]  will create a CloudFormation stack for cluster itself and 2 nodegroup stack(s)
2023-02-04 20:16:16 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2023-02-04 20:16:16 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --cluster=spark-on-eks'
2023-02-04 20:16:16 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "spark-on-eks" in "ap-northeast-2"
2023-02-04 20:16:16 [ℹ]  CloudWatch logging will not be enabled for cluster "spark-on-eks" in "ap-northeast-2"
2023-02-04 20:16:16 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=spark-on-eks'
2023-02-04 20:16:16 [ℹ]
2 sequential tasks: { create cluster control plane "spark-on-eks",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        2 parallel sub-tasks: {
            create nodegroup "spark-ng-x86",
            create nodegroup "spark-ng-arm",
        },
    }
}
2023-02-04 20:16:16 [ℹ]  building cluster stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:16:17 [ℹ]  deploying stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:16:47 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:17:18 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"
2023-02-04 20:18:19 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-cluster"

2023-02-04 20:29:25 [ℹ]  building nodegroup stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:25 [ℹ]  building nodegroup stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  deploying stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-arm"
2023-02-04 20:29:25 [ℹ]  deploying stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:25 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"
2023-02-04 20:29:56 [ℹ]  waiting for CloudFormation stack "eksctl-spark-on-eks-nodegroup-spark-ng-x86"

2023-02-04 20:34:19 [ℹ]  waiting for the control plane to become ready
2023-02-04 20:34:19 [✔]  saved kubeconfig as "/Users/soonbeom/.kube/config"
2023-02-04 20:34:19 [ℹ]  no tasks
2023-02-04 20:34:19 [✔]  all EKS cluster resources for "spark-on-eks" have been created
2023-02-04 20:34:19 [ℹ]  adding identity "arn:aws:iam::509076023497:role/eksctl-spark-on-eks-nodegroup-spa-NodeInstanceRole-1H8KFQM987K7X" to auth ConfigMap
2023-02-04 20:34:19 [ℹ]  nodegroup "spark-ng-x86" has 0 node(s)
2023-02-04 20:34:19 [ℹ]  waiting for at least 2 node(s) to become ready in "spark-ng-x86"
2023-02-04 20:35:10 [ℹ]  nodegroup "spark-ng-x86" has 3 node(s)
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-33-80.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-8-112.ap-northeast-2.compute.internal" is not ready
2023-02-04 20:35:10 [ℹ]  node "ip-192-168-80-19.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:10 [ℹ]  adding identity "arn:aws:iam::509076023497:role/eksctl-spark-on-eks-nodegroup-spa-NodeInstanceRole-FIYEKMFETV5I" to auth ConfigMap
2023-02-04 20:35:10 [ℹ]  nodegroup "spark-ng-arm" has 0 node(s)
2023-02-04 20:35:10 [ℹ]  waiting for at least 2 node(s) to become ready in "spark-ng-arm"
2023-02-04 20:35:42 [ℹ]  nodegroup "spark-ng-arm" has 3 node(s)
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-22-60.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-35-245.ap-northeast-2.compute.internal" is ready
2023-02-04 20:35:42 [ℹ]  node "ip-192-168-78-81.ap-northeast-2.compute.internal" is not ready
2023-02-04 20:35:43 [ℹ]  kubectl command should work with "/Users/soonbeom/.kube/config", try 'kubectl get nodes'
2023-02-04 20:35:44 [✔]  EKS cluster "spark-on-eks" in "ap-northeast-2" region is ready
```
VPC 를 비롯한 서브넷, 시큐리티 그룹 등과 같은 AWS 리소스가 자동으로 생성된다.

[참고] 클러스터 삭제 명령어
```
$ eksctl delete cluster -f cluster.yaml
```



### 4. 클러스터 생성 확인 ###
```
$ kubectl config get-contexts
CURRENT   NAME                                             CLUSTER                                 AUTHINFO                                         NAMESPACE
          docker-desktop                                   docker-desktop                          docker-desktop
*         name-of-account@spark-on-eks.ap-northeast-2.eksctl.io   spark-on-eks.ap-northeast-2.eksctl.io   hopigaga@spark-on-eks.ap-northeast-2.eksctl.io

$ kubectl get nodes
NAME                                                STATUS   ROLES    AGE     VERSION
ip-192-168-22-60.ap-northeast-2.compute.internal    Ready    <none>   3m15s   v1.24.9-eks-49d8fe8
ip-192-168-33-80.ap-northeast-2.compute.internal    Ready    <none>   4m9s    v1.24.9-eks-49d8fe8
ip-192-168-35-245.ap-northeast-2.compute.internal   Ready    <none>   3m17s   v1.24.9-eks-49d8fe8
ip-192-168-78-81.ap-northeast-2.compute.internal    Ready    <none>   3m19s   v1.24.9-eks-49d8fe8
ip-192-168-8-112.ap-northeast-2.compute.internal    Ready    <none>   4m8s    v1.24.9-eks-49d8fe8
ip-192-168-80-19.ap-northeast-2.compute.internal    Ready    <none>   4m8s    v1.24.9-eks-49d8fe8

$ kubectl get pods -o wide
No resources found in default namespace.
(base) soonbeom@bcd07468d10a ~ % kubectl get pods -o wide -A
NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE     IP               NODE                                                NOMINATED NODE   READINESS GATES
kube-system   aws-node-2ctq9            1/1     Running   0          4m26s   192.168.80.19    ip-192-168-80-19.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-5g79d            1/1     Running   0          3m33s   192.168.22.60    ip-192-168-22-60.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-5mmgm            1/1     Running   0          4m26s   192.168.8.112    ip-192-168-8-112.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-9xlzx            1/1     Running   0          3m37s   192.168.78.81    ip-192-168-78-81.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-krqth            1/1     Running   0          4m27s   192.168.33.80    ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   aws-node-xxw47            1/1     Running   0          3m35s   192.168.35.245   ip-192-168-35-245.ap-northeast-2.compute.internal   <none>           <none>
kube-system   coredns-dc4979556-7zhhh   1/1     Running   0          15m     192.168.62.173   ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   coredns-dc4979556-hgkv9   1/1     Running   0          15m     192.168.50.214   ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-5jlgq          1/1     Running   0          4m26s   192.168.8.112    ip-192-168-8-112.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-8q7rx          1/1     Running   0          4m27s   192.168.33.80    ip-192-168-33-80.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-mdn6w          1/1     Running   0          3m33s   192.168.22.60    ip-192-168-22-60.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-mtq7n          1/1     Running   0          3m35s   192.168.35.245   ip-192-168-35-245.ap-northeast-2.compute.internal   <none>           <none>
kube-system   kube-proxy-qzvrh          1/1     Running   0          3m37s   192.168.78.81    ip-192-168-78-81.ap-northeast-2.compute.internal    <none>           <none>
kube-system   kube-proxy-wz7jd          1/1     Running   0          4m26s   192.168.80.19    ip-192-168-80-19.ap-northeast-2.compute.internal    <none>           <none>
(base) soonbeom@bcd07468d10a ~ %
```

self-managed 노드 그룹의 경우, AWS 콘솔의 노드 그룹 리스트에 정보가 나오지 않는다.

![](https://github.com/gnosia93/spark-on-eks/blob/main/images/eks-cluster.png)



## 참고자료 ##
* https://eksctl.io/usage/creating-and-managing-clusters/
* https://stackoverflow.com/questions/64059038/the-results-of-aws-eks-list-nodegroups-and-eksctl-get-nodegroups-are-inconsi
* https://findstar.pe.kr/2022/08/21/which-instance-type-is-right-for-EKS/
